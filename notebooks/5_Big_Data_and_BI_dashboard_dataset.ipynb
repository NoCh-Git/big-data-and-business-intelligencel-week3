{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data & BI — Feature Engineering\n",
    "## Notebook 5: Dashboard-Ready Dataset\n",
    "\n",
    "This notebook puts everything together:\n",
    "- rebuild + clean\n",
    "- apply rules\n",
    "- select columns\n",
    "- export to CSV (or to a table) for dashboarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Start with raw messy data (same as Notebook 1 & 2)\n",
    "data = {\n",
    "    \"order_id\":   [1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010],\n",
    "    \"order_date\": [\"2025-01-03\", \"2025/01/03\", \"03-01-2025\", \"2025-01-04\", None,\n",
    "                    \"2025-01-05\", \"2025-01-05\", \"2025-01-06\", \"2025-01-06\", \"2025-01-06\"],\n",
    "    \"customer_id\": [501, 502, 503, 503, 504, 505, 506, 506, 507, None],\n",
    "    \"country\":    [\"DE\", \"Germany\", \"germany\", \"FR\", \"France\", \"DE\", \"DE \", \"?\", None, \"GER\"],\n",
    "    \"product\":    [\"Widget A\", \"Widget B\", \"Widget A\", \"Widget C\", \"Widget A\",\n",
    "                    \"Widget B\", \"Widget B\", \"Widget C\", \"Widget A\", \"Widget A\"],\n",
    "    \"quantity\":   [2, 1, 3, 1, -1, 2, 2, 1, 5, 2],\n",
    "    \"unit_price\": [20.0, 35.5, 20.0, 50.0, 20.0, None, 35.5, 50.0, 20.0, 20.0],\n",
    "    \"discount\":   [0.0, 0.1, None, 0.0, 0.0, 0.05, 0.0, None, 0.0, 0.0],\n",
    "    \"channel\":    [\"online\", \"Online\", \"offline\", \"partner\", \"online\",\n",
    "                    \"offline\", \"online \", \"ONLINE\", None, \"partner\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"=== APPLYING CLEANING PIPELINE FROM NOTEBOOKS 1-4 ===\\n\")\n",
    "\n",
    "# 1) Parse dates (Notebook 2)\n",
    "df[\"order_date\"] = pd.to_datetime(df[\"order_date\"], errors=\"coerce\")\n",
    "print(f\"[1] Dates parsed: {df['order_date'].notna().sum()}/{len(df)} valid\")\n",
    "\n",
    "# 2) Standardize categoricals (Notebook 2)\n",
    "df[\"country_clean\"] = df[\"country\"].str.lower().str.strip().map({\n",
    "    \"de\": \"germany\", \"germany\": \"germany\", \"ger\": \"germany\",\n",
    "    \"fr\": \"france\", \"france\": \"france\", \"?\": \"unknown\", None: \"unknown\"\n",
    "}).fillna(df[\"country\"].str.lower().str.strip())\n",
    "\n",
    "df[\"channel_clean\"] = df[\"channel\"].str.lower().str.strip().map({\n",
    "    \"online\": \"online\", \"offline\": \"offline\", \"partner\": \"partner\", None: \"unknown\", \"\": \"unknown\"\n",
    "}).fillna(df[\"channel\"].str.lower().str.strip())\n",
    "print(f\"[2] Countries standardized: {df['country_clean'].unique()}\")\n",
    "print(f\"[3] Channels standardized: {df['channel_clean'].unique()}\")\n",
    "\n",
    "# 3) Impute numeric columns (Notebook 2)\n",
    "median_price = df[\"unit_price\"].median()\n",
    "df[\"unit_price_filled\"] = df[\"unit_price\"].fillna(median_price)\n",
    "df[\"discount_filled\"] = df[\"discount\"].fillna(0.0)\n",
    "print(f\"[4] Prices imputed with median: {median_price}\")\n",
    "\n",
    "# 4) Fix negative quantity (Notebook 3)\n",
    "df[\"quantity_fixed\"] = df[\"quantity\"].abs()\n",
    "print(f\"[5] Negative quantity fixed: {(df['quantity_fixed'] > 0).all()}\")\n",
    "\n",
    "# 5) Fill customer_id with unique high-value IDs (Notebook 2)\n",
    "missing_mask = df[\"customer_id\"].isna()\n",
    "num_missing = missing_mask.sum()\n",
    "max_existing_id = df[\"customer_id\"].max()\n",
    "if pd.notna(max_existing_id):\n",
    "    order_of_magnitude = 10 ** (math.floor(math.log10(max_existing_id)) + 2)\n",
    "    placeholder_start = order_of_magnitude\n",
    "else:\n",
    "    placeholder_start = 999000001\n",
    "df[\"customer_id_clean\"] = df[\"customer_id\"].copy()\n",
    "df.loc[missing_mask, \"customer_id_clean\"] = range(placeholder_start, placeholder_start + num_missing)\n",
    "df[\"customer_id_clean\"] = df[\"customer_id_clean\"].astype(int)\n",
    "print(f\"[6] Customer IDs filled: placeholder {placeholder_start} used\")\n",
    "\n",
    "# 6) Calculate revenue (Notebook 3 & 4)\n",
    "df[\"revenue\"] = df[\"quantity_fixed\"] * df[\"unit_price_filled\"] * (1 - df[\"discount_filled\"])\n",
    "print(f\"[7] Revenue calculated: mean = €{df['revenue'].mean():.2f}\")\n",
    "\n",
    "print(\"\\n=== CLEANED DATA PREVIEW ===\")\n",
    "df[[\"order_id\", \"order_date\", \"customer_id_clean\", \"country_clean\", \"product\", \n",
    "    \"quantity_fixed\", \"unit_price_filled\", \"discount_filled\", \"channel_clean\", \"revenue\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Choose final columns\n",
    "\n",
    "### When to keep vs. drop columns in BI:\n",
    "\n",
    "**Keep original + cleaned columns when:**\n",
    "- Auditing/debugging is needed (compare before/after transformations)\n",
    "- Regulatory compliance requires tracking changes\n",
    "- Data lineage documentation is important\n",
    "- Multiple teams use different versions of the same field\n",
    "\n",
    "**Drop original messy columns when:**\n",
    "- Delivering final dataset to end users (avoid confusion)\n",
    "- Dashboard performance matters (fewer columns = faster queries)\n",
    "- Storage/memory is limited\n",
    "- Original values have no business value after cleaning\n",
    "\n",
    "**For this dashboard dataset, we drop originals because:**\n",
    "1. End users don't need to see \"DE\" vs \"germany\" - just the cleaned value\n",
    "2. Dashboards will be faster with fewer columns\n",
    "3. We already documented the transformations in Notebooks 1-3 for auditing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing dates (critical for time series dashboards)\n",
    "final_df = df.dropna(subset=[\"order_date\"]).copy()\n",
    "\n",
    "# Select final columns for dashboard\n",
    "# We keep only cleaned columns and rename them for simplicity\n",
    "final_df = final_df[[\n",
    "    \"order_id\", \n",
    "    \"order_date\", \n",
    "    \"customer_id_clean\",\n",
    "    \"country_clean\", \n",
    "    \"product\", \n",
    "    \"channel_clean\",\n",
    "    \"quantity_fixed\", \n",
    "    \"unit_price_filled\", \n",
    "    \"discount_filled\", \n",
    "    \"revenue\"\n",
    "]].rename(columns={\n",
    "    \"customer_id_clean\": \"customer_id\",\n",
    "    \"country_clean\": \"country\", \n",
    "    \"channel_clean\": \"channel\",\n",
    "    \"quantity_fixed\": \"quantity\",\n",
    "    \"unit_price_filled\": \"unit_price\",\n",
    "    \"discount_filled\": \"discount\"\n",
    "})\n",
    "\n",
    "print(f\"Final dataset: {len(final_df)} rows × {len(final_df.columns)} columns\")\n",
    "print(f\"Dropped {len(df) - len(final_df)} rows with missing dates\")\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. final checks\n",
    "Make sure there are no nulls in important columns and dtypes are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== NULL CHECK ===\")\n",
    "print(final_df.isna().sum())\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(final_df.dtypes)\n",
    "print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "print(final_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f96ed4",
   "metadata": {},
   "source": [
    "## 3. Export to CSV for dashboard tools\n",
    "\n",
    "This cleaned dataset can be imported into:\n",
    "- Power BI / Tableau\n",
    "- Excel pivot tables\n",
    "- SQL database\n",
    "- Cloud data warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0875534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV in the data folder\n",
    "output_path = \"../data/dashboard_ready.csv\"\n",
    "final_df.to_csv(output_path, index=False)\n",
    "print(f\"Dashboard-ready dataset exported to: {output_path}\")\n",
    "print(f\"  - {len(final_df)} rows\")\n",
    "print(f\"  - {len(final_df.columns)} columns\")\n",
    "print(f\"  - File size: {final_df.memory_usage(deep=True).sum() / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3abca3e",
   "metadata": {},
   "source": [
    "## Summary: What we accomplished\n",
    "\n",
    "This notebook combined all cleaning steps from Notebooks 1-4:\n",
    "\n",
    "1. **Notebook 1**: Defined the cleaning strategy\n",
    "2. **Notebook 2**: Implemented standardization and imputation\n",
    "   - Parsed mixed date formats\n",
    "   - Standardized country/channel labels\n",
    "   - Imputed missing prices with median (20.0)\n",
    "   - Filled missing customer IDs with unique high-value placeholders (10000+)\n",
    "3. **Notebook 3**: Applied validation and fixed data issues\n",
    "   - Fixed negative quantity (took absolute value)\n",
    "   - Validated technical requirements\n",
    "   - Applied business rules\n",
    "4. **Notebook 4**: Calculated KPIs\n",
    "   - Revenue after discount\n",
    "   - Aggregations by country/channel\n",
    "5. **Notebook 5 (this one)**: Created final dashboard dataset\n",
    "   - Applied entire pipeline in sequence\n",
    "   - Removed rows with missing dates (1 row)\n",
    "   - Exported clean CSV for BI tools\n",
    "\n",
    "**Result**: 9 rows × 10 columns, ready for dashboarding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
