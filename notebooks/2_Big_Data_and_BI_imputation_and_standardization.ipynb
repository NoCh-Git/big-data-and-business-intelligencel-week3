{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data & BI — Feature Engineering\n",
    "## Notebook 2: Imputation & Standardization\n",
    "\n",
    "Now we **apply** the cleaning plan from notebook 1:\n",
    "- parse dates with mixed formats\n",
    "- standardize country/channel labels\n",
    "- impute missing numeric fields\n",
    "- create cleaned columns we can rely on later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    \"order_id\":   [1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010],\n",
    "    \"order_date\": [\"2025-01-03\", \"2025/01/03\", \"03-01-2025\", \"2025-01-04\", None,\n",
    "                    \"2025-01-05\", \"2025-01-05\", \"2025-01-06\", \"2025-01-06\", \"2025-01-06\"],\n",
    "    \"customer_id\": [501, 502, 503, 503, 504, 505, 506, 506, 507, None],\n",
    "    \"country\":    [\"DE\", \"Germany\", \"germany\", \"FR\", \"France\", \"DE\", \"DE \", \"?\", None, \"GER\"],\n",
    "    \"product\":    [\"Widget A\", \"Widget B\", \"Widget A\", \"Widget C\", \"Widget A\",\n",
    "                    \"Widget B\", \"Widget B\", \"Widget C\", \"Widget A\", \"Widget A\"],\n",
    "    \"quantity\":   [2, 1, 3, 1, -1, 2, 2, 1, 5, 2],\n",
    "    \"unit_price\": [20.0, 35.5, 20.0, 50.0, 20.0, None, 35.5, 50.0, 20.0, 20.0],\n",
    "    \"discount\":   [0.0, 0.1, None, 0.0, 0.0, 0.05, 0.0, None, 0.0, 0.0],\n",
    "    \"channel\":    [\"online\", \"Online\", \"offline\", \"partner\", \"online\",\n",
    "                    \"offline\", \"online \", \"ONLINE\", None, \"partner\"]\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parse messy dates → datetime\n",
    "We allow errors and convert them to NaT, we'll decide later whether to drop or impute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"order_date\"] = pd.to_datetime(df[\"order_date\"], errors=\"coerce\")\n",
    "df[[\"order_id\", \"order_date\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standardize text columns\n",
    "We do lowercase + strip and then map weird values to a standard form.\n",
    "\n",
    "**Two approaches:**\n",
    "1. **Keep both columns** (what we do here) → safer for auditing, you can compare before/after\n",
    "2. **Replace directly** → saves memory, cleaner final dataframe\n",
    "\n",
    "We keep both so you can verify the transformations worked correctly. In the final preview (section 5), we'll drop the original messy columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country\n",
    "# Step 1: Convert to lowercase and remove leading/trailing spaces\n",
    "df[\"country_clean\"] = df[\"country\"].str.lower().str.strip()\n",
    "\n",
    "# Step 2: Define a mapping dictionary (key = messy value, value = standard form)\n",
    "country_map = {\n",
    "    \"de\": \"germany\",\n",
    "    \"germany\": \"germany\",\n",
    "    \"ger\": \"germany\",\n",
    "    \"fr\": \"france\",\n",
    "    \"france\": \"france\",\n",
    "    \"?\": \"unknown\",\n",
    "    None: \"unknown\"\n",
    "}\n",
    "\n",
    "# Step 3: Apply the mapping\n",
    "# .map(country_map) → replaces values according to the dictionary\n",
    "# .fillna(df[\"country_clean\"]) → if a value is NOT in the map, keep the original cleaned value\n",
    "df[\"country_clean\"] = df[\"country_clean\"].map(country_map).fillna(df[\"country_clean\"])\n",
    "\n",
    "# channel\n",
    "# Step 1: Convert to lowercase and remove leading/trailing spaces\n",
    "df[\"channel_clean\"] = df[\"channel\"].str.lower().str.strip()\n",
    "\n",
    "# Step 2: Define a mapping dictionary\n",
    "channel_map = {\n",
    "    \"online\": \"online\",\n",
    "    \"offline\": \"offline\",\n",
    "    \"partner\": \"partner\",\n",
    "    \"\": \"unknown\",\n",
    "    None: \"unknown\"\n",
    "}\n",
    "\n",
    "# Step 3: Apply the mapping\n",
    "# .map(channel_map) → replaces values according to the dictionary\n",
    "# .fillna(df[\"channel_clean\"]) → if a value is NOT in the map, keep the original cleaned value\n",
    "df[\"channel_clean\"] = df[\"channel_clean\"].map(channel_map).fillna(df[\"channel_clean\"])\n",
    "\n",
    "# Display the results: show original and cleaned versions side-by-side\n",
    "# .drop_duplicates() → remove duplicate rows to see unique transformations only\n",
    "# .reset_index(drop=True) → renumber rows from 0, 1, 2...\n",
    "df[[\"country\", \"country_clean\", \"channel\", \"channel_clean\"]].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc3d15",
   "metadata": {},
   "source": [
    "Let's now have another look at the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3baf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe42169",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What is Imputation?\n",
    "\n",
    "**Imputation** = filling in missing values with estimated/reasonable values instead of deleting rows.\n",
    "\n",
    "### Why do we need imputation?\n",
    "\n",
    "In real-world data, missing values are common:\n",
    "- Sensors fail → missing temperature readings\n",
    "- Users skip survey questions → missing demographic data\n",
    "- System errors → missing prices or dates\n",
    "\n",
    "### Two main approaches:\n",
    "\n",
    "#### 1. **Delete rows with missing values** (simple but wasteful)\n",
    "- Pro: Clean data, no assumptions\n",
    "- Con: Lose potentially valuable information\n",
    "- Con: Can create bias (what if missing values aren't random?)\n",
    "- **Use when**: Very few missing values (<5%) and they're truly random\n",
    "\n",
    "#### 2. **Impute (fill) missing values** (more sophisticated)\n",
    "- Pro: Keep all rows, preserve data volume\n",
    "- Pro: Can maintain statistical properties\n",
    "- Con: Introduces assumptions/estimates\n",
    "- **Use when**: Missing values are common or systematic\n",
    "\n",
    "### Common Imputation Strategies:\n",
    "\n",
    "| Strategy | How it works | When to use | Example |\n",
    "|----------|-------------|-------------|---------|\n",
    "| **Constant** | Fill with a fixed value | Categorical data, safe defaults | Fill missing country with \"unknown\" |\n",
    "| **Mean** | Fill with average of all values | Numerical, normally distributed | Average temperature |\n",
    "| **Median** | Fill with middle value | Numerical, with outliers | Prices (robust to extreme values) |\n",
    "| **Mode** | Fill with most common value | Categorical data | Most common product category |\n",
    "| **Forward/Backward fill** | Use previous/next value | Time series data | Stock prices |\n",
    "| **Group-specific** | Fill based on category | When patterns differ by group | Price by product type |\n",
    "| **Model-based** | Predict using ML | Complex relationships | Predict based on multiple features |\n",
    "\n",
    "### For our BI use case:\n",
    "- **Numeric fields** (price, discount) → median or mean (median is safer with outliers)\n",
    "- **Categorical fields** (country, channel) → constant like \"unknown\" or mode\n",
    "- **IDs** (customer_id) → single placeholder value like -1 or \"UNKNOWN\" or unique placeholder value if our values might be used as a primary key or identification way.\n",
    "- **Dates** → depends on context (might need to delete if critical for time series)\n",
    "\n",
    "\n",
    "## Note\n",
    "**Important**: Always document what imputation you did! Your stakeholders need to know which values are real vs. estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Impute numeric columns\n",
    "- price → median (simple, robust)\n",
    "- discount → 0.0 (safe default in BI)\n",
    "- quantity → we'll fix negative in next notebook, so we keep it for now\n",
    "\n",
    "**Why median for missing price?**\n",
    "- We have ONE missing price (row with order_id 1006, Widget B)\n",
    "- We could look up \"Widget B costs 35.5\" BUT that only works if we know the product\n",
    "- **Median is a general strategy** that works even when we don't know the product or don't have a price lookup table\n",
    "- In real BI pipelines, you often can't manually check each missing value → automated imputation is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ca9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see which row has missing price\n",
    "print(\"Row with missing price:\")\n",
    "print(df[df[\"unit_price\"].isna()][[\"order_id\", \"product\", \"unit_price\"]])\n",
    "\n",
    "print(\"\\nAll prices by product:\")\n",
    "print(df.groupby(\"product\")[\"unit_price\"].describe())\n",
    "\n",
    "print(f\"\\nMedian of ALL prices: {df['unit_price'].median()}\")\n",
    "print(\"\\nSo we'll fill the missing Widget B price with 20.0 (the median)\")\n",
    "print(\"A BETTER approach would be to use product-specific prices, but median is simpler and more general.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_price = df[\"unit_price\"].median()\n",
    "df[\"unit_price_filled\"] = df[\"unit_price\"].fillna(median_price)\n",
    "df[\"discount_filled\"] = df[\"discount\"].fillna(0.0)\n",
    "df[[\"unit_price\", \"unit_price_filled\", \"discount\", \"discount_filled\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Customer IDs\n",
    "\n",
    "**Problem**: We have missing customer IDs, but customer_id might be a primary key!\n",
    "\n",
    "**Three approaches:**\n",
    "\n",
    "1. **Single placeholder** (e.g., -1 or 0) → Simple but loses uniqueness\n",
    "   - Can't use as primary key anymore\n",
    "   - Can't distinguish between different unknown customers\n",
    "   \n",
    "2. **Unique negative IDs** (e.g., -1, -2, -3...) → Preserves uniqueness\n",
    "   - Can still use as primary key\n",
    "   - Negative IDs can cause issues in SQL (foreign key constraints, WHERE clauses, confusion)\n",
    "   - May break business logic that assumes IDs > 0\n",
    "\n",
    "3. **Unique IDs outside normal range** (e.g., 999000001, 999000002...) → Best practice\n",
    "   - Can still use as primary key\n",
    "   - No negative number issues in SQL/BI tools\n",
    "   - Easy to identify as placeholder (start with 999...)\n",
    "   - Won't conflict with real customer IDs (assuming they're < 999000000)\n",
    "\n",
    "We'll use **unique high-value IDs (999000001+)** - the safest approach for production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Single placeholder (BAD for primary keys)\n",
    "# df[\"customer_id_clean\"] = df[\"customer_id\"].fillna(-1).astype(int)\n",
    "\n",
    "# Option 2: Unique negative IDs (OK but can cause SQL issues)\n",
    "# missing_mask = df[\"customer_id\"].isna()\n",
    "# num_missing = missing_mask.sum()\n",
    "# df[\"customer_id_clean\"] = df[\"customer_id\"].copy()\n",
    "# df.loc[missing_mask, \"customer_id_clean\"] = range(-1, -1 - num_missing, -1)\n",
    "\n",
    "# Option 3: Unique high-value IDs (BEST for production)\n",
    "# Dynamically find the safe range based on actual data\n",
    "missing_mask = df[\"customer_id\"].isna()\n",
    "num_missing = missing_mask.sum()\n",
    "\n",
    "# Find the maximum existing customer ID to avoid conflicts\n",
    "max_existing_id = df[\"customer_id\"].max()\n",
    "print(f\"Maximum existing customer_id: {max_existing_id}\")\n",
    "\n",
    "# Choose a placeholder start that's clearly outside the normal range\n",
    "# Strategy: round up to next order of magnitude and add safety margin\n",
    "# E.g., if max is 507, use 10000 or 100000\n",
    "import math\n",
    "if pd.notna(max_existing_id):\n",
    "    # Calculate order of magnitude and go to next higher one\n",
    "    order_of_magnitude = 10 ** (math.floor(math.log10(max_existing_id)) + 2)\n",
    "    placeholder_start = order_of_magnitude\n",
    "else:\n",
    "    # If all IDs are missing, start from a safe default\n",
    "    placeholder_start = 999000001\n",
    "\n",
    "print(f\"Placeholder IDs will start from: {placeholder_start}\")\n",
    "\n",
    "# Generate unique placeholder IDs\n",
    "df[\"customer_id_clean\"] = df[\"customer_id\"].copy()\n",
    "df.loc[missing_mask, \"customer_id_clean\"] = range(placeholder_start, placeholder_start + num_missing)\n",
    "df[\"customer_id_clean\"] = df[\"customer_id_clean\"].astype(int)\n",
    "\n",
    "# Verify: check if customer_id_clean is unique (can be used as primary key)\n",
    "print(f\"\\nTotal rows: {len(df)}\")\n",
    "print(f\"Unique customer_id_clean values: {df['customer_id_clean'].nunique()}\")\n",
    "print(f\"Is unique (can be primary key)? {df['customer_id_clean'].is_unique}\")\n",
    "print(f\"Placeholder IDs used: {sorted(df.loc[missing_mask, 'customer_id_clean'].values)}\")\n",
    "\n",
    "df[[\"customer_id\", \"customer_id_clean\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preview cleaned frame\n",
    "This is what we'll feed into the next notebook for integrity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_cols = [\n",
    "    \"order_id\", \"order_date\", \"customer_id_clean\", \"country_clean\",\n",
    "    \"product\", \"quantity\", \"unit_price_filled\", \"discount_filled\", \"channel_clean\"\n",
    "]\n",
    "df_clean = df[clean_cols].copy()\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff10a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63df1a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
